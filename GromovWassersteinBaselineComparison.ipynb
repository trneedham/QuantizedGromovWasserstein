{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gromov-Wasserstein Baseline Comparison\n",
    "\n",
    "In this notebook, we give a simple baseline comparison of the performance of the quantized Gromov-Wasserstein (qGW) algorithm to standard Gromov-Wasserstein (GW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import ot\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from random import sample, uniform\n",
    "\n",
    "from quantizedGW import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "We will construct toy datasets using the `sklearn` function `make_blobs`. These will consist of 2D point clouds with a varying number of points. Each point cloud is considered as a metric measure space (mm-space) with Euclidean distance and uniform measure.\n",
    "\n",
    "Given two point clouds, we match using the GW algorithm and the qGW algorithm. The algorithms output couplings $\\mu_{GW}$ and $\\mu_{qGW}$, respectively. We also construct a product coupling $\\mu_{prod}$. We use $\\mu_{prod}$ as the putative maximizer of GW loss and $\\mu_{GW}$ as the putative minimizer. We construct a relative error of $\\mu_{qGW}$ as\n",
    "$$\n",
    "\\mathrm{rel. error} = \\frac{\\mu_{qGW}-\\mu_{GW}}{\\mu_{prod} - \\mu_{GW}}\n",
    "$$\n",
    "\n",
    "For each dataset size, we run this trial numerous times and report the average relative error, as well as the average compute time. We do this for a variety of sampling rates in the qGW algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [200,400,600,800,1000,1200,1400,1600,1800,2000] # Data sizes\n",
    "variance = 0.2 # Randomize the sizes a bit\n",
    "num_trials = 5 # How many trials to run for each size\n",
    "sample_rates = [0.1,0.2,0.3,0.4,0.5] # Sample rates for qGW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_gw = {}\n",
    "results_qgw = {}\n",
    "\n",
    "\n",
    "for mean_points in means:\n",
    "    print('Starting',mean_points,'points...')\n",
    "    \n",
    "    times_gw = []\n",
    "    losses_gw = []\n",
    "    \n",
    "    times_qgw = np.zeros([len(sample_rates),num_trials])\n",
    "    losses_qgw = np.zeros([len(sample_rates),num_trials])\n",
    "    relative_losses = np.zeros([len(sample_rates),num_trials])\n",
    "    \n",
    "    for j in range(num_trials):\n",
    "\n",
    "        # Create Datasets\n",
    "        num_points1 = int(random.uniform((1-variance)*mean_points,(1+variance)*mean_points))\n",
    "        num_points2 = int(random.uniform((1-variance)*mean_points,(1+variance)*mean_points))\n",
    "        n_features1 = 2\n",
    "        n_features2 = 2\n",
    "\n",
    "        X1, y = make_blobs(n_samples=num_points1, n_features = n_features1)\n",
    "        Dist1 = euclidean_distances(X1)\n",
    "\n",
    "        X2, y = make_blobs(n_samples=num_points2, n_features = n_features2)\n",
    "        Dist2 = euclidean_distances(X2)\n",
    "\n",
    "        p1 = ot.unif(num_points1)\n",
    "        p2 = ot.unif(num_points2)\n",
    "        \n",
    "        product_loss = gwloss_init(Dist1,Dist2,p1,p2,p1[:,None]*p2[None,:])\n",
    "\n",
    "        ## GW Coupling\n",
    "        start = time.time()\n",
    "        coup_gw, log = ot.gromov.gromov_wasserstein(\n",
    "            Dist1, Dist2, p1, p2, 'square_loss', verbose=False, log=True)\n",
    "        times_gw.append(time.time() - start)\n",
    "\n",
    "        gw_loss = gwloss_init(Dist1,Dist2,p1,p2,coup_gw)\n",
    "        losses_gw.append(gw_loss)\n",
    "        \n",
    "\n",
    "        ## quantized GW with random subset selection\n",
    "        for (i,rate) in enumerate(sample_rates):\n",
    "            samples = int(rate*min([num_points1,num_points2]))\n",
    "            node_subset1 = list(set(sample(list(range(num_points1)),samples)))\n",
    "            node_subset2 = list(set(sample(list(range(num_points2)),samples)))\n",
    "\n",
    "            start = time.time()\n",
    "            coup_comp = compressed_gw_point_cloud(Dist1,Dist2,p1,p2,\n",
    "                                                  node_subset1,node_subset2,\n",
    "                                                  verbose = False,return_dense = True)\n",
    "            times_qgw[i,j] = time.time() - start\n",
    "            \n",
    "            quantized_loss = gwloss_init(Dist1,Dist2,p1,p2,coup_comp)\n",
    "            losses_qgw[i,j] = quantized_loss\n",
    "            \n",
    "            relative_losses[i,j] = (quantized_loss - gw_loss)/(product_loss - gw_loss)\n",
    "            \n",
    "        print('Trial',j,'done')\n",
    "    \n",
    "    mean_time_gw = np.mean(times_gw)\n",
    "    mean_loss_gw = np.mean(losses_gw)\n",
    "    results_gw[mean_points] = {'time':mean_time_gw,'loss':mean_loss_gw} \n",
    "\n",
    "    mean_times_comp = np.mean(times_qgw, axis = 1)\n",
    "    mean_losses_comp = np.mean(losses_qgw,axis = 1)\n",
    "    mean_relative_losses = np.mean(relative_losses, axis = 1)\n",
    "    results_qgw[mean_points] = {'time':mean_times_comp,'loss':mean_losses_comp,'relative loss':mean_relative_losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_times = [results_gw[mean_points]['time'] for mean_points in means]\n",
    "qgw_times = [[results_qgw[mean_points]['time'][j] for mean_points in means] for j in range(len(sample_rates))]\n",
    "rel_losses = [[results_qgw[mean_points]['relative loss'][j] for mean_points in means] for j in range(len(sample_rates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = means[:-1]\n",
    "labels = means[:-1]\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "for j in range(len(sample_rates)):\n",
    "    plt.plot(x,qgw_times[j][:-1],label = str(sample_rates[j]))\n",
    "plt.plot(x,gw_times[:-1],'--',label = 'GW')\n",
    "# plt.plot(x,gw_ent_1_times,'-.',label = 'GW Ent 10')\n",
    "# plt.plot(x,gw_ent_2_times,':',label = 'GW Ent 100')\n",
    "\n",
    "fontsize = 18\n",
    "\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel('Avg. Points per Dataset',fontsize = fontsize)\n",
    "plt.ylabel('Avg. Time (s)',fontsize = fontsize)\n",
    "plt.legend(loc=\"upper left\",fontsize = fontsize)\n",
    "plt.title('Matching Datasets: Compute Time',fontsize = fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = means[:-1]\n",
    "labels = means[:-1]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "for j in range(len(sample_rates)):\n",
    "    plt.plot(x,[100*rel_losses[j][k] for k in range(len(rel_losses[j]))][:-1],label = str(sample_rates[j]))\n",
    "plt.plot(x,[0 for j in range(len(gw_times))][:-1],'--',label = 'GW')\n",
    "\n",
    "fontsize = 18\n",
    "\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel('Avg. Points per Dataset',fontsize = fontsize)\n",
    "plt.ylabel('Avg. Relative Error (%)',fontsize = fontsize)\n",
    "plt.legend(loc=\"upper right\",fontsize = fontsize)\n",
    "plt.title('Matching Datasets: Relative Error Against GW',fontsize = fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Figure\n",
    "\n",
    "To verify that the good performance of qGW w.r.t. relative error reflects high quality matchings, we can plot the matchings obtained by GW and qGW.\n",
    "\n",
    "To visualize a matching, we color the source point cloud (say, by distance to a given point) and transfer the color to the target point cloud using the coupling matrix for each method. The transferred color is the weighted average of the colors matched to a given vertex, with weights coming from the coupling matrix values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new datasets for matching\n",
    "num_points1 = 2000\n",
    "num_points2 = 2000\n",
    "n_features1 = 2\n",
    "n_features2 = 2\n",
    "\n",
    "X1, y = make_blobs(n_samples=num_points1, n_features = n_features1)\n",
    "Dist1 = euclidean_distances(X1)\n",
    "\n",
    "X2, y = make_blobs(n_samples=num_points2, n_features = n_features2)\n",
    "Dist2 = euclidean_distances(X2)\n",
    "\n",
    "p1 = ot.unif(num_points1)\n",
    "p2 = ot.unif(num_points2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GW Coupling\n",
    "start = time.time()\n",
    "coup_gw, log = ot.gromov.gromov_wasserstein(\n",
    "    Dist1, Dist2, p1, p2, 'square_loss', verbose=False, log=True)\n",
    "time_gw = time.time() - start\n",
    "print('GW Compute Time:',time_gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## quantized GW with random subset selection\n",
    "sample_rate = .2\n",
    "samples = int(sample_rate*num_points1)\n",
    "node_subset1 = list(set(sample(list(range(num_points1)),samples)))\n",
    "node_subset2 = list(set(sample(list(range(num_points2)),samples)))\n",
    "\n",
    "start = time.time()\n",
    "coup_qgw= compressed_gw_point_cloud(Dist1,Dist2,p1,p2,node_subset1,node_subset2,verbose = True,return_dense = True)\n",
    "time_qgw = time.time() - start\n",
    "print('Compressed GW Compute Time:', time_qgw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_loss = gwloss_init(Dist1,Dist2,p1,p2,coup_qgw)\n",
    "gw_loss = gwloss_init(Dist1,Dist2,p1,p2,coup_gw)\n",
    "product_loss = gwloss_init(Dist1,Dist2,p1,p2,p1[:,None]*p2[None,:])\n",
    "\n",
    "print('Loss with Compression:', quantized_loss)\n",
    "print('Loss without Compression:', gw_loss)\n",
    "print('Product Coupling Loss:', product_loss)\n",
    "\n",
    "rel_error = (quantized_loss - gw_loss)/(product_loss - gw_loss)*100\n",
    "\n",
    "print('Relative Error w.r.t. product and optimal (percent):', rel_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "xs = X1[:,0]\n",
    "ys = X1[:,1]\n",
    "\n",
    "# Color by distance to the given point\n",
    "point = 1\n",
    "c1 = Dist1[point,:]\n",
    "\n",
    "fontsize = 14\n",
    "ax1.scatter(xs, ys, c = c1)\n",
    "plt.axis('equal')\n",
    "plt.title('Source Data', fontsize = fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "xs = X2[:,0]\n",
    "ys = X2[:,1]\n",
    "\n",
    "c2 = [np.dot(coup_gw[:,j],c1)/np.sum(coup_gw[:,j]) for j in range(Dist2.shape[0])]\n",
    "\n",
    "ax1.scatter(xs, ys, c = c2)\n",
    "plt.axis('equal')\n",
    "plt.title('Target Data, GW Matching \\n Compute Time '+str(np.round(time_gw,2))+'s', fontsize = fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "xs = X2[:,0]\n",
    "ys = X2[:,1]\n",
    "\n",
    "c2 = [np.dot(coup_qgw[:,j],c1)/np.sum(coup_qgw[:,j]) for j in range(Dist2.shape[0])]\n",
    "\n",
    "ax1.scatter(xs, ys, c = c2)\n",
    "plt.axis('equal')\n",
    "plt.title('Target Data, Compressed GW Matching \\n Compute Time '+str(np.round(time_qgw,2))+'s, '+ str(np.round(rel_error,1))+'% Rel. Error', fontsize = fontsize)\n",
    "\n",
    "plt.savefig('Matching_Blobs_Target_Comp',dpi = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
